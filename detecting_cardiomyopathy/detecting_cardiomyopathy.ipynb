{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932dd2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bac2e6",
   "metadata": {},
   "source": [
    "# Problem Overview\n",
    "Cardiomyopathy is a disease that weakens the heart muscle. This makes it harder for your heart to pump blood.\n",
    "In this classification problem we`ll try to detect this disease using one of machine learning methods - K-Nearest-Neighbours\n",
    "(if you need further information about this method: https://scikit-learn.org/stable/modules/neighbors.html).\n",
    "In the directory there is file called task_data which contains data of different hearts.\n",
    "Numbers in second columns state whether heart is diseased or not:\n",
    "**1 (diseased heart)**,\n",
    "**0 (healthy heart)**  \n",
    "  \n",
    "K-Nearest-Neighbours method will be used to learn from data and predict outcomes for new cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c36d69",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7247816",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading data into pandas DataFrame\n",
    "'''\n",
    "data = pd.read_csv('task_data.csv')   \n",
    "\n",
    "\n",
    "'''\n",
    "Let's see that certain columns have ',' as a separator instead of '.' so we need to exchange it\n",
    "'''\n",
    "def dot_for_comma(data):\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        return data\n",
    "\n",
    "    data['Inscribed circle radius'] = data['Inscribed circle radius'].str.replace(',', '.')\n",
    "    data['Heart perimeter'] = data['Heart perimeter'].str.replace(',', '.')\n",
    "    data['CTR - Cardiothoracic Ratio'] = data['CTR - Cardiothoracic Ratio'].str.replace(',', '.')\n",
    "    return data\n",
    "\n",
    "\n",
    "data = dot_for_comma(data)\n",
    "    \n",
    "\n",
    "\n",
    "'''\n",
    "We create 2 Dataframes: first with our features, and second with outcomes\n",
    "'''\n",
    "X = data.drop('Cardiomegaly', axis = 1)    \n",
    "Y = data['Cardiomegaly'].copy()\n",
    "\n",
    "'''\n",
    "Let`s split our dataset into training and tested data\n",
    "parameter named 'random state' makes the data divided into permanent groups\n",
    "'''\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.28, random_state=42) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4212c968",
   "metadata": {},
   "source": [
    "### GridSearchCV\n",
    "GridSearchCV is one of better tools to be used in machine learning. \"It is particularly useful for hyperparameter tuning, where the goal is to find the best combination of parameters that result in the highest model performance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be48d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class sklearn.model_selection.GridSearchCV(estimator, param_grid, *, scoring=None, n_jobs=None, refit=True, cv=None,\n",
    "verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n",
    "\n",
    "How to choose scoring parameter?\n",
    "First of all we need to define if we have classification or regression problem. In our case we're checking wheter a person\n",
    "is diseased or not. Therefore it wil be classification problem. In short:\n",
    "- accuracy tells us how many tests were right (in %)\n",
    "\n",
    "- precision : (people who were actually deseased/people who model marked as deseased) \n",
    "  (e.g. people who model mark as deseased: 30, people who were actually deseased: 25, people marked by model as healthy: 5 \n",
    "  -> precision scoring: 25/30 = 83% )\n",
    "\n",
    "- recall :  (people who were actually deseased and marked as deseased) / (all of people that were deseased)\n",
    "(e.g. people who model mark as deseased and who were actually deseased: 25 (as above). \n",
    "However the exact number of deseased people were higher: 35, so: recall scoring = 25/35 = 71%)\n",
    "\n",
    "- f1 is between precision and recall\n",
    "'''\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"model__n_neighbors\": [3, 5],  # Number of neighbors to consider\n",
    "    \"model__weights\": [\"uniform\", \"distance\"],   # How neighbors contribute to the prediction (distance means that points who are\n",
    "                                                 # closer have bigger impact)\n",
    "    \"model__metric\": [\"minkowski\", \"manhattan\", \"euclidean\", \"chebyshev\"],  # Distance metrics to test\n",
    "                                                                            # Different ways to measure distance between points\n",
    "}\n",
    "\n",
    "\n",
    "'''\n",
    "Pipeline is similar as before\n",
    "'''\n",
    "pipe_knn = ImbPipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),  \n",
    "    (\"smote\", SMOTE(random_state=42, k_neighbors=2)), # creates artificial samples for minority class\n",
    "    (\"model\", KNeighborsClassifier())       \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # shuffling here makes huge difference\n",
    "# because the data is mixed before splitting\n",
    "\n",
    "'''\n",
    "It's time to initialize Grid Search for our model, which will check every possible \n",
    "combination given in our param_grid dictionary.\n",
    "'''\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator = pipe_knn,\n",
    "    param_grid = param_grid,\n",
    "    scoring = 'f1',     \n",
    "    verbose=1,              # Display progress in the console (values can be from 0 to 3 - it depends on how much info you want to see)\n",
    "    cv = cv_strategy,\n",
    "    n_jobs=-1               # it engages all your processes to make the processing faster (other values: -2,1,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efefda27",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e95d0",
   "metadata": {},
   "source": [
    "### GridSearch learning on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f094845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best parameters: {'model__metric': 'manhattan', 'model__n_neighbors': 5, 'model__weights': 'distance'}\n",
      "Best accuracy (averaged CV): 83.2 %\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best accuracy (averaged CV): {(grid_search.best_score_*100):.1f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3665c9",
   "metadata": {},
   "source": [
    "### Checking results for best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64599b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores: [0.8  0.75 0.86 1.   0.89]\n",
      "Mean: 0.859\n",
      "Standard deviation: 0.085\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We can use pipeline from imblearn.pipeline to speed up all the processes. It's better than classic pipeline because \n",
    "it has SMOTE method which create artificial samples for minority class if we do not have enough data \n",
    "'''\n",
    "\n",
    "pipe_knn = ImbPipeline(steps=[    \n",
    "    (\"scaler\", StandardScaler()),     # Standardizing features is crucial in machine learning to get outcome based on all of them (features)\n",
    "                                      # (if there were a huge difference between values, the result may depend on only one feature)       \n",
    "    \n",
    "    (\"model\", KNeighborsClassifier(   \n",
    "        n_neighbors = 5,              # Number of neighbors used to predict value (less is better for little amount of data)\n",
    "        \n",
    "        weights='distance',           # Weight controls how much influence each neighbor has when making a prediction.                                      \n",
    "                                      # closer neighbors of a query point will have a greater influence than neighbors which are further away.        \n",
    "       \n",
    "        metric='manhattan'            \n",
    "    ))\n",
    "])\n",
    "\n",
    "'''\n",
    "Avoid this line below before using cross_val_score (it can lead to data leakage and scores can be higher)\n",
    "You need to use it after cross_val_score\n",
    "'''\n",
    "#pipe_knn.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "'''\n",
    "First param: estimator: model to fit the data\n",
    "cv: how many divisions (folds) are to be made\n",
    "It's important that fitting data is inside cross_val_score()\n",
    "cross_val_score() returns array of scores of the estimator for each run of the cross validation.\n",
    "'''\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # Not only does it divide but also shuffles\n",
    "cv_scores = cross_val_score(pipe_knn , x_train, y_train, cv=cv_strategy, scoring='f1')\n",
    "print(f\"CV scores: {np.round(cv_scores, 2)}\")\n",
    "print(f\"Mean: {np.mean(cv_scores):.3f}\")\n",
    "print(f\"Standard deviation: {np.std(cv_scores):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f74b308",
   "metadata": {},
   "source": [
    "### Final outcome on tested data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4b96c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.7 % of result accuracy\n"
     ]
    }
   ],
   "source": [
    "pipe_knn.fit(x_train, y_train)  \n",
    "outcome = pipe_knn.predict(x_test)\n",
    "print(f\"{(accuracy_score(y_test, outcome)*100):.1f} % of result accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99069c1",
   "metadata": {},
   "source": [
    "\n",
    "| type of model          |   KNN    |\n",
    "|------------------------|----------|\n",
    "| accuracy of best model |  84.2%   |\n",
    "| result accuracy        |  72.7%   |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_AGH",
   "language": "python",
   "name": "ai_agh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
